accuracy,avg_latency_s,dataset,em,evidence_f1_macro,evidence_f1_micro,evidence_precision_micro,evidence_recall_micro,examples_evaluated,f1,fever_label_mode,fever_score,inclusion_rate,note,output_file,per_label_metrics,response_f1,response_precision,response_recall,span_prf_avg,span_prf_computed,split
0.0,0.2656170077389106,mwong/fever-evidence-related,,0.0,,,,10,,binary,0.0,,,selfrag_eval_smoke/fever_outputs.jsonl,"{'0': {'p': None, 'r': None, 'f1': None, 'examples': 7}, '1': {'p': None, 'r': None, 'f1': None, 'examples': 3}}",,,,,,train
,,microsoft/ms_marco,,,,,,0,,,,,No evaluable examples found (no ranked retrieval fields detected).,selfrag_eval_smoke/microsoft_ms_marco_outputs.jsonl,,,,,,,validation
,0.7137281094677747,squad_v2,0.4,,,,,10,0.1151496847734466,,,,,selfrag_eval_smoke/squad_v2_outputs.jsonl,,,,,,,validation
,0.5369620006298647,hotpotqa/hotpot_qa__distractor,0.7,,,,,10,0.1043896833587236,,,,,selfrag_eval_smoke/hotpotqa_hotpot_qa__distractor_outputs.jsonl,,,,,,,validation
,0.45980320340022446,hotpotqa/hotpot_qa__fullwiki,0.2,,,,,10,0.05948930296756384,,,,,selfrag_eval_smoke/hotpotqa_hotpot_qa__fullwiki_outputs.jsonl,,,,,,,validation
,0.8119183295872062,wandb/RAGTruth-processed,,,,,,10,,,,,,selfrag_eval_smoke/ragtruth_outputs.jsonl,,0.0,0.0,0.0,,False,train
,0.2901597588090226,mandarjoshi/trivia_qa::rc,,,,,,10,,,,0.3,,selfrag_eval_smoke/mandarjoshi_trivia_qa__rc_outputs.jsonl,,,,,,,validation
,0.29320725447032603,sentence-transformers/natural-questions,0.0,,,,,10,0.07205489593553246,,,,,selfrag_eval_smoke/sentence-transformers_natural-questions_outputs.jsonl,,,,,,,train
