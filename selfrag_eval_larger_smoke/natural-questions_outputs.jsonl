{"index": 5622, "question": {"text": "when did smoking get banned in public places", "tokens": ["when", "did", "smoking", "get", "banned", "in", "public", "places"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1988[Partially supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1988[Partially supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1063557737506926}
{"index": 7718, "question": {"text": "when did the lion king start on broadway", "tokens": ["when", "did", "the", "lion", "king", "start", "on", "broadway"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1997.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1997.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1063557737506926}
{"index": 855, "question": {"text": "describe the procedure of ambulance services in this community", "tokens": ["describe", "the", "procedure", "of", "ambulance", "services", "in", "this", "community"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]Describe the procedure of ambulance services in this community.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]Describe the procedure of ambulance services in this community.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1063557737506926}
{"index": 5221, "question": {"text": "who played danny's brother in law on blue bloods", "tokens": ["who", "played", "danny", "'s", "brother", "in", "law", "on", "blue", "bloods"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]who played danny's brother in law on blue bloods - Frank Reagan (Kevin Dillon)[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]who played danny's brother in law on blue bloods - Frank Reagan (Kevin Dillon)[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1063557737506926}
{"index": 6144, "question": {"text": "what type of houses did the pequot live in", "tokens": ["what", "type", "of", "houses", "did", "the", "pequot", "live", "in"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]type of houses did the pequot live in[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]type of houses did the pequot live in[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.0698720840155147}
{"index": 7321, "question": {"text": "what was the key factor that allowed hershey and chase", "tokens": ["what", "was", "the", "key", "factor", "that", "allowed", "hershey", "and", "chase"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]the key factor that allowed hershey and chase[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]the key factor that allowed hershey and chase[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.0698720840155147}
{"index": 2754, "question": {"text": "what observation led to the theory of seafloor spreading", "tokens": ["what", "observation", "led", "to", "the", "theory", "of", "seafloor", "spreading"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]what observation led to the theory of seafloor spreading?[Utility:1]", "pred_short": "Retrieval]<paragraph>[Irrelevant]what observation led to the theory of seafloor spreading?[Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.0698720840155147}
{"index": 1500, "question": {"text": "when was coffee first made into a drink", "tokens": ["when", "was", "coffee", "first", "made", "into", "a", "drink"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]15th century[Utility:1]", "pred_short": "Retrieval]<paragraph>[Irrelevant]15th century[Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.0698720840155147}
{"index": 2085, "question": {"text": "what is the term for circular movement around a central point", "tokens": ["what", "is", "the", "term", "for", "circular", "movement", "around", "a", "central", "point"]}, "prediction_raw": "SHORT_ANSWER: circular movement around a central point.[Utility:5]", "pred_short": "circular movement around a central point.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.5465724517707713}
{"index": 6168, "question": {"text": "why were the wars between rome and carthage called the punic wars", "tokens": ["why", "were", "the", "wars", "between", "rome", "and", "carthage", "called", "the", "punic", "wars"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]The Punic Wars were named after the Carthaginians, who were called \"Punics\" by the Romans.[Fully supported][Continue to Use Evidence]The wars were named after the Carthaginians because they were the main opponents in the conflicts.[Continue to Use Evidence]The name \"Punic\" was derived from the Latin word \"Punica,\" which was used to refer to the Carthaginians.[Continue to Use Evidence]The name was used to distinguish the Carthaginians from the Romans, who were known as \"Latins.\"[Continue to Use Evidence]The name \"Punic\" was also used to refer to the language spoken by the Carthaginians, which was a form of Phoenician.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]The Punic Wars were named after the Carthaginians, who were called \"Punics\" by the Romans.[Fully supported][Continue to Use Evidence]The wars were named after the Carthaginians because they were the main opponents in the conflicts.[Continue to Use Evidence]The name \"Punic\" was derived from the Latin word \"Punica,\" which was used to refer to the Carthaginians.[Continue to Use Evidence]The name was used to distinguish the Carthaginians from the Romans, who were known as \"Latins.\"[Continue to Use Evidence]The name \"Punic\" was also used to refer to the language spoken by the Carthaginians, which was a form of Phoenician.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.5465724517707713}
{"index": 6359, "question": {"text": "what is a e8 in the marine corps", "tokens": ["what", "is", "a", "e8", "in", "the", "marine", "corps"]}, "prediction_raw": "SHORT_ANSWER: E8 is a rank in the United States Marine Corps.[Continue to Use Evidence]It is equivalent to a sergeant major in the Army and the Air Force, and a chief petty officer in the Navy.[Continue to Use Evidence]The rank is typically held by a senior non-commissioned officer who has demonstrated exceptional leadership and technical expertise in their field.[Continue to Use Evidence]The Marine Corps uses a different system for ranking its officers and enlisted members than the other military branches.[Retrieval]<paragraph>[Relevant]LONG_ANSWER: E8 is a rank in the United States Marine Corps.[No support / Contradictory][Continue to Use Evidence]It is equivalent to a sergeant major in the Army and the Air Force, and a chief petty officer in the Navy.[Continue to Use Evidence]The rank is typically held by a senior non-commissioned officer who has demonstrated exceptional leadership and technical expertise in their field.[Continue to Use Evidence]The Marine Corps uses a different system for ranking its officers and enlisted members than the other military branches.[Utility:5]", "pred_short": "E8 is a rank in the United States Marine Corps.[Continue to Use Evidence]It is equivalent to a sergeant major in the Army and the Air Force, and a chief petty officer in the Navy.[Continue to Use Evidence]The rank is typically held by a senior non-commissioned officer who has demonstrated exceptional leadership and technical expertise in their field.[Continue to Use Evidence]The Marine Corps uses a different system for ranking its officers and enlisted members than the other military branches.[Retrieval]<paragraph>[Relevant", "pred_long": "E8 is a rank in the United States Marine Corps.[No support / Contradictory][Continue to Use Evidence]It is equivalent to a sergeant major in the Army and the Air Force, and a chief petty officer in the Navy.[Continue to Use Evidence]The rank is typically held by a senior non-commissioned officer who has demonstrated exceptional leadership and technical expertise in their field.[Continue to Use Evidence]The Marine Corps uses a different system for ranking its officers and enlisted members than the other military branches.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.5465724517707713}
{"index": 7698, "question": {"text": "what is the processor for google nexus 5", "tokens": ["what", "is", "the", "processor", "for", "google", "nexus", "5"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The processor for Google Nexus 5 is a Qualcomm Snapdragon 800.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The processor for Google Nexus 5 is a Qualcomm Snapdragon 800.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.5465724517707713}
{"index": 5338, "question": {"text": "when was the land rover defender first built", "tokens": ["when", "was", "the", "land", "rover", "defender", "first", "built"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1983[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1983[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.23295498004881665}
{"index": 4479, "question": {"text": "who played joey's agent estelle on friends", "tokens": ["who", "played", "joey", "'s", "agent", "estelle", "on", "friends"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]who played joey's agent estelle on friends[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]who played joey's agent estelle on friends[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.23295498004881665}
{"index": 1298, "question": {"text": "where did the name florida georgia line come from", "tokens": ["where", "did", "the", "name", "florida", "georgia", "line", "come", "from"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]The name Florida Georgia Line came from the state of Florida and the state of Georgia, where the band members are from.[No Retrieval]LONG_ANSWER: The name Florida Georgia Line came from the state of Florida and the state of Georgia, where the band members are from.[Continue to Use Evidence]The band's name was inspired by the state line that separates the two states.[Utility:5]", "pred_short": "No Retrieval]The name Florida Georgia Line came from the state of Florida and the state of Georgia, where the band members are from.[No Retrieval", "pred_long": "The name Florida Georgia Line came from the state of Florida and the state of Georgia, where the band members are from.[Continue to Use Evidence]The band's name was inspired by the state line that separates the two states.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.23295498004881665}
{"index": 5868, "question": {"text": "has the reverse transcriptase survived the nucleotide synthesis", "tokens": ["has", "the", "reverse", "transcriptase", "survived", "the", "nucleotide", "synthesis"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]yes[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]yes[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.23295498004881665}
{"index": 2227, "question": {"text": "who is the original singer of i write sins not tragedies", "tokens": ["who", "is", "the", "original", "singer", "of", "i", "write", "sins", "not", "tragedies"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]Daniel Isaac McGuffey\n\nLONG_ANSWER: Daniel McGuffey is an American singer-songwriter and musician.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]Daniel Isaac McGuffey", "pred_long": "Daniel McGuffey is an American singer-songwriter and musician.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.27025843947194517}
{"index": 5499, "question": {"text": "do you have to read james bond books in order", "tokens": ["do", "you", "have", "to", "read", "james", "bond", "books", "in", "order"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]No, you do not have to read James Bond books in order.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]No, you do not have to read James Bond books in order.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.27025843947194517}
{"index": 61, "question": {"text": "what is the main mineral in lithium batteries", "tokens": ["what", "is", "the", "main", "mineral", "in", "lithium", "batteries"]}, "prediction_raw": "SHORT_ANSWER: The main mineral in lithium batteries is lithium.[Retrieval]<paragraph>[Relevant]LONG_ANSWER: Lithium is the main mineral in lithium batteries.[No support / Contradictory][Continue to Use Evidence]Lithium is a light, silvery-white, and soft alkali metal that is highly reactive and flammable.[Continue to Use Evidence]It is the least dense of all metals and has the lowest melting point of all metals.[Utility:5]", "pred_short": "The main mineral in lithium batteries is lithium.[Retrieval]<paragraph>[Relevant", "pred_long": "Lithium is the main mineral in lithium batteries.[No support / Contradictory][Continue to Use Evidence]Lithium is a light, silvery-white, and soft alkali metal that is highly reactive and flammable.[Continue to Use Evidence]It is the least dense of all metals and has the lowest melting point of all metals.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.27025843947194517}
{"index": 1543, "question": {"text": "what was the name of the plant on little shop of horrors", "tokens": ["what", "was", "the", "name", "of", "the", "plant", "on", "little", "shop", "of", "horrors"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]The Plant.[Utility:5]", "pred_short": "No Retrieval]The Plant.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.27025843947194517}
{"index": 5389, "question": {"text": "where do they drop in game of games", "tokens": ["where", "do", "they", "drop", "in", "game", "of", "games"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]in game of games[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]in game of games[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1172059724922292}
{"index": 7072, "question": {"text": "who was the wrestler who died in the ring", "tokens": ["who", "was", "the", "wrestler", "who", "died", "in", "the", "ring"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]The wrestler who died in the ring was Chris Benoit.[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]The wrestler who died in the ring was Chris Benoit.[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1172059724922292}
{"index": 4638, "question": {"text": "when did the movie karate kid come out", "tokens": ["when", "did", "the", "movie", "karate", "kid", "come", "out"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1984.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1984.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1172059724922292}
{"index": 1075, "question": {"text": "what are the different types of non communicable disease", "tokens": ["what", "are", "the", "different", "types", "of", "non", "communicable", "disease"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]The different types of non-communicable disease are chronic diseases, genetic diseases, and lifestyle-related diseases.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]The different types of non-communicable disease are chronic diseases, genetic diseases, and lifestyle-related diseases.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1172059724922292}
{"index": 3965, "question": {"text": "who gained fame for her actions during the american civil war", "tokens": ["who", "gained", "fame", "for", "her", "actions", "during", "the", "american", "civil", "war"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]famous american civil war figures[No support / Contradictory][Utility:1]", "pred_short": "Retrieval]<paragraph>[Relevant]famous american civil war figures[No support / Contradictory][Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1366890967474319}
{"index": 7187, "question": {"text": "how does the cash cab guy read the questions", "tokens": ["how", "does", "the", "cash", "cab", "guy", "read", "the", "questions"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The Cash Cab guy reads the questions by looking at the monitor.[No Retrieval]LONG_ANSWER: The Cash Cab guy reads the questions by looking at the monitor.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The Cash Cab guy reads the questions by looking at the monitor.[No Retrieval", "pred_long": "The Cash Cab guy reads the questions by looking at the monitor.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.1366890967474319}
{"index": 4507, "question": {"text": "what is the purpose of a continuing resolution", "tokens": ["what", "is", "the", "purpose", "of", "a", "continuing", "resolution"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]To fund the government.[Utility:1]", "pred_short": "Retrieval]<paragraph>[Irrelevant]To fund the government.[Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1366890967474319}
{"index": 6602, "question": {"text": "battlestar galactica season 1 episode 1 starts with previously", "tokens": ["battlestar", "galactica", "season", "1", "episode", "1", "starts", "with", "previously"]}, "prediction_raw": "SHORT_ANSWER: previously[Utility:5]", "pred_short": "previously[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1366890967474319}
{"index": 4034, "question": {"text": "who sang the song i hear you knocking", "tokens": ["who", "sang", "the", "song", "i", "hear", "you", "knocking"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]who sang the song i hear you knocking?[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]who sang the song i hear you knocking?[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14145823172293603}
{"index": 1756, "question": {"text": "if a star is class b what is its temperature", "tokens": ["if", "a", "star", "is", "class", "b", "what", "is", "its", "temperature"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]its temperature[Utility:5]", "pred_short": "No Retrieval]its temperature[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14145823172293603}
{"index": 2832, "question": {"text": "how long is ex on the beach on for", "tokens": ["how", "long", "is", "ex", "on", "the", "beach", "on", "for"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]for[Utility:5]", "pred_short": "No Retrieval]for[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14145823172293603}
{"index": 2081, "question": {"text": "beginning to be a bit of an acronym", "tokens": ["beginning", "to", "be", "a", "bit", "of", "an", "acronym"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]beginning to be a bit of an acronym\nLONG_ANSWER: The sentence is about the process of becoming an acronym.[No Retrieval]The process is beginning to take place.[Utility:5]", "pred_short": "No Retrieval]beginning to be a bit of an acronym", "pred_long": "The sentence is about the process of becoming an acronym.[No Retrieval]The process is beginning to take place.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.14145823172293603}
{"index": 997, "question": {"text": "when does finding your roots air on pbs", "tokens": ["when", "does", "finding", "your", "roots", "air", "on", "pbs"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]Finding Your Roots airs on PBS on Fridays at 8:00 PM.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]Finding Your Roots airs on PBS on Fridays at 8:00 PM.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 1.4012610880308785}
{"index": 6001, "question": {"text": "how many hoops are used in a game of croquet", "tokens": ["how", "many", "hoops", "are", "used", "in", "a", "game", "of", "croquet"]}, "prediction_raw": "SHORT_ANSWER: 5\nLONG_ANSWER: There are 5 hoops used in a game of croquet.[Utility:5]", "pred_short": "5", "pred_long": "There are 5 hoops used in a game of croquet.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 1.4012610880308785}
{"index": 6347, "question": {"text": "european exploration in the 15th and 16th centuries", "tokens": ["european", "exploration", "in", "the", "15th", "and", "16th", "centuries"]}, "prediction_raw": "SHORT_ANSWER: European exploration in the 15th and 16th centuries.[Utility:5]", "pred_short": "European exploration in the 15th and 16th centuries.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 1.4012610880308785}
{"index": 113, "question": {"text": "list of states of australia and their capitals", "tokens": ["list", "of", "states", "of", "australia", "and", "their", "capitals"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]Australia is a country located in Oceania.[Continue to Use Evidence]It consists of six states and two territories.[Continue to Use Evidence]The capital city of each state is listed below:\n\nNew South Wales: Sydney\nVictoria: Melbourne\nQueensland: Brisbane\nSouth Australia: Adelaide\nTasmania: Hobart\nWestern Australia: Perth\nNorthern Territory: Darwin\nAustralian Capital Territory: Canberra\nNew Zealand: Wellington\nPapua New Guinea: Port Moresby\nFiji: Suva\nSolomon Islands: Honiara\nVanuatu: Port Vila\nKiribati: Tarawa\nTuvalu: Funafuti\nNauru: Yaren District\nMarshall Islands: Majuro\nMicronesia: Palikir\nPalau: Ngerulmud\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNor", "pred_short": "No Retrieval]Australia is a country located in Oceania.[Continue to Use Evidence]It consists of six states and two territories.[Continue to Use Evidence]The capital city of each state is listed below:\n\nNew South Wales: Sydney\nVictoria: Melbourne\nQueensland: Brisbane\nSouth Australia: Adelaide\nTasmania: Hobart\nWestern Australia: Perth\nNorthern Territory: Darwin\nAustralian Capital Territory: Canberra\nNew Zealand: Wellington\nPapua New Guinea: Port Moresby\nFiji: Suva\nSolomon Islands: Honiara\nVanuatu: Port Vila\nKiribati: Tarawa\nTuvalu: Funafuti\nNauru: Yaren District\nMarshall Islands: Majuro\nMicronesia: Palikir\nPalau: Ngerulmud\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNor", "pred_long": "New South Wales: Sydney\nVictoria: Melbourne\nQueensland: Brisbane\nSouth Australia: Adelaide\nTasmania: Hobart\nWestern Australia: Perth\nNorthern Territory: Darwin\nAustralian Capital Territory: Canberra\nNew Zealand: Wellington\nPapua New Guinea: Port Moresby\nFiji: Suva\nSolomon Islands: Honiara\nVanuatu: Port Vila\nKiribati: Tarawa\nTuvalu: Funafuti\nNauru: Yaren District\nMarshall Islands: Majuro\nMicronesia: Palikir\nPalau: Ngerulmud\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Marianas: Saipan\nPuerto Rico: San Juan\nAmerican Samoa: Pago Pago\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nPalau: Ngerulmud\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNorthern Mariana Islands: Saipan\nCaroline Islands: Palau\nWake Island: Wake Island\nGuam: Hagatna\nNor", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 1.4012610880308785}
{"index": 224, "question": {"text": "how tall is the actor who plays hagrid in harry potter", "tokens": ["how", "tall", "is", "the", "actor", "who", "plays", "hagrid", "in", "harry", "potter"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The actor who plays Hagrid in Harry Potter is 7 feet tall.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The actor who plays Hagrid in Harry Potter is 7 feet tall.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.08045642002252862}
{"index": 7424, "question": {"text": "what areas of africa were involved in colonization", "tokens": ["what", "areas", "of", "africa", "were", "involved", "in", "colonization"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]areas of africa were involved in colonization[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]areas of africa were involved in colonization[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.08045642002252862}
{"index": 4902, "question": {"text": "what college does everyone in gossip girl go to", "tokens": ["what", "college", "does", "everyone", "in", "gossip", "girl", "go", "to"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]Columbia University[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]Columbia University[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.08045642002252862}
{"index": 1024, "question": {"text": "who has man city drawn with this season", "tokens": ["who", "has", "man", "city", "drawn", "with", "this", "season"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]Manchester City[Utility:1]", "pred_short": "Retrieval]<paragraph>[Irrelevant]Manchester City[Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.08045642002252862}
{"index": 7375, "question": {"text": "the area enclosed by hysteresis loop is a measure of", "tokens": ["the", "area", "enclosed", "by", "hysteresis", "loop", "is", "a", "measure", "of"]}, "prediction_raw": "SHORT_ANSWER: area enclosed by hysteresis loop[Utility:5]", "pred_short": "area enclosed by hysteresis loop[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14183627924649045}
{"index": 41, "question": {"text": "lynyrd skynyrd last of a dying breed album", "tokens": ["lynyrd", "skynyrd", "last", "of", "a", "dying", "breed", "album"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]lynyrd skynyrd last of a dying breed album[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]lynyrd skynyrd last of a dying breed album[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14183627924649045}
{"index": 1446, "question": {"text": "what written material is included in the talmud", "tokens": ["what", "written", "material", "is", "included", "in", "the", "talmud"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]what written material is included in the talmud[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]what written material is included in the talmud[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14183627924649045}
{"index": 7097, "question": {"text": "who plays dawn on nicky ricky dicky and dawn", "tokens": ["who", "plays", "dawn", "on", "nicky", "ricky", "dicky", "and", "dawn"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]Nicky, Ricky, Dicky, and Dawn is an American sitcom that aired on Nickelodeon from 2009 to 2018.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]Nicky, Ricky, Dicky, and Dawn is an American sitcom that aired on Nickelodeon from 2009 to 2018.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.14183627924649045}
{"index": 7565, "question": {"text": "who was the movie citizen kane based on", "tokens": ["who", "was", "the", "movie", "citizen", "kane", "based", "on"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]Citizen Kane is a 1941 American drama film by Orson Welles.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]Citizen Kane is a 1941 American drama film by Orson Welles.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.09293429600074887}
{"index": 4026, "question": {"text": "the story of lover's leap in jamaica", "tokens": ["the", "story", "of", "lover", "'s", "leap", "in", "jamaica"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]the story of lover's leap in jamaica[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]the story of lover's leap in jamaica[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.09293429600074887}
{"index": 5612, "question": {"text": "who sings the songs in crazy ex girlfriend", "tokens": ["who", "sings", "the", "songs", "in", "crazy", "ex", "girlfriend"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]who sings the songs in crazy ex girlfriend is Rachel Bloom.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]who sings the songs in crazy ex girlfriend is Rachel Bloom.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.09293429600074887}
{"index": 892, "question": {"text": "when does nathan get in a car accident", "tokens": ["when", "does", "nathan", "get", "in", "a", "car", "accident"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]when does nathan get in a car accident?[Utility:5]", "pred_short": "No Retrieval]when does nathan get in a car accident?[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.09293429600074887}
{"index": 5194, "question": {"text": "when is the world consumer right day celebrated", "tokens": ["when", "is", "the", "world", "consumer", "right", "day", "celebrated"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]World Consumer Rights Day is celebrated on 15 March every year.[Continue to Use Evidence]It is an international day that marks the anniversary of the founding of the consumer movement in 1983.[Continue to Use Evidence]It is an opportunity for consumer groups to raise awareness of consumer rights and to campaign for them.[Continue to Use Evidence]The day is also used to promote a better understanding of consumer rights and to encourage people to take action to protect them.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]World Consumer Rights Day is celebrated on 15 March every year.[Continue to Use Evidence]It is an international day that marks the anniversary of the founding of the consumer movement in 1983.[Continue to Use Evidence]It is an opportunity for consumer groups to raise awareness of consumer rights and to campaign for them.[Continue to Use Evidence]The day is also used to promote a better understanding of consumer rights and to encourage people to take action to protect them.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2720409977482632}
{"index": 5679, "question": {"text": "how much square feet is a basketball court", "tokens": ["how", "much", "square", "feet", "is", "a", "basketball", "court"]}, "prediction_raw": "SHORT_ANSWER: 1,000 square feet.[No Retrieval]LONG_ANSWER: A basketball court is a rectangular indoor or outdoor area where basketball is played.[Retrieval]<paragraph>", "pred_short": "1,000 square feet.[No Retrieval", "pred_long": "A basketball court is a rectangular indoor or outdoor area where basketball is played.[Retrieval]<paragraph>", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.2720409977482632}
{"index": 7749, "question": {"text": "lemony snicket's a series of unfortunate events hindi", "tokens": ["lemony", "snicket", "'s", "a", "series", "of", "unfortunate", "events", "hindi"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]lemony snicket's a series of unfortunate events hindi[Utility:5]", "pred_short": "No Retrieval]lemony snicket's a series of unfortunate events hindi[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2720409977482632}
{"index": 2441, "question": {"text": "where is the wailing wall and why is it a holy spot", "tokens": ["where", "is", "the", "wailing", "wall", "and", "why", "is", "it", "a", "holy", "spot"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The Wailing Wall is a section of the Western Wall in Jerusalem.[Continue to Use Evidence]It is a holy spot because it is believed to be the remnants of the Second Temple of Jerusalem, which was destroyed in 70 CE.[Continue to Use Evidence]It is also considered a holy site because it is believed to be the place where Jews can come to pray and ask for forgiveness.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The Wailing Wall is a section of the Western Wall in Jerusalem.[Continue to Use Evidence]It is a holy spot because it is believed to be the remnants of the Second Temple of Jerusalem, which was destroyed in 70 CE.[Continue to Use Evidence]It is also considered a holy site because it is believed to be the place where Jews can come to pray and ask for forgiveness.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2720409977482632}
{"index": 5340, "question": {"text": "which glass slipper did cinderella leave behind at the ball", "tokens": ["which", "glass", "slipper", "did", "cinderella", "leave", "behind", "at", "the", "ball"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]glass slipper[Utility:5]", "pred_short": "No Retrieval]glass slipper[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.05410895374370739}
{"index": 4101, "question": {"text": "where are muscle cells found in the body", "tokens": ["where", "are", "muscle", "cells", "found", "in", "the", "body"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]in the body[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]in the body[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.05410895374370739}
{"index": 2133, "question": {"text": "where did the rulers of the qing dynasty originate", "tokens": ["where", "did", "the", "rulers", "of", "the", "qing", "dynasty", "originate"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]the Manchu people[Fully supported][Utility:1]", "pred_short": "Retrieval]<paragraph>[Relevant]the Manchu people[Fully supported][Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.05410895374370739}
{"index": 6958, "question": {"text": "where does dividends go on cash flow statement", "tokens": ["where", "does", "dividends", "go", "on", "cash", "flow", "statement"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]on cash flow statement[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]on cash flow statement[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.05410895374370739}
{"index": 1520, "question": {"text": "where was the rise of the planet of the apes filmed", "tokens": ["where", "was", "the", "rise", "of", "the", "planet", "of", "the", "apes", "filmed"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]Vancouver, British Columbia.[Utility:5]", "pred_short": "No Retrieval]Vancouver, British Columbia.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2604419729905203}
{"index": 3072, "question": {"text": "when did they film season 10 of drag race", "tokens": ["when", "did", "they", "film", "season", "10", "of", "drag", "race"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]They filmed season 10 of drag race in 2018.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]They filmed season 10 of drag race in 2018.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2604419729905203}
{"index": 6801, "question": {"text": "s p jain school of global management world ranking", "tokens": ["s", "p", "jain", "school", "of", "global", "management", "world", "ranking"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]s p jain school of global management world ranking\nLONG_ANSWER:[Retrieval]<paragraph><paragraph><paragraph><paragraph><paragraph>\ns p jain school of global management world ranking\nLONG_ANSWER: s p jain school of global management world ranking is the ranking of the s p jain school of global management, a business school in india, in various national and international global rankings.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]s p jain school of global management world ranking", "pred_long": "[Retrieval]<paragraph><paragraph><paragraph><paragraph><paragraph>\ns p jain school of global management world ranking\nLONG_ANSWER: s p jain school of global management world ranking is the ranking of the s p jain school of global management, a business school in india, in various national and international global rankings.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.2604419729905203}
{"index": 1729, "question": {"text": "when did the east india company take control of india", "tokens": ["when", "did", "the", "east", "india", "company", "take", "control", "of", "india"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1858.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1858.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2604419729905203}
{"index": 540, "question": {"text": "tqm is designed to build quality into the product and process design", "tokens": ["tqm", "is", "designed", "to", "build", "quality", "into", "the", "product", "and", "process", "design"]}, "prediction_raw": "SHORT_ANSWER: tqm is designed to build quality into the product and process design.[Utility:5]", "pred_short": "tqm is designed to build quality into the product and process design.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06791525450535119}
{"index": 4071, "question": {"text": "who began the age of exploration in portugal", "tokens": ["who", "began", "the", "age", "of", "exploration", "in", "portugal"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]who began the age of exploration in portugal[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]who began the age of exploration in portugal[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06791525450535119}
{"index": 7471, "question": {"text": "when was the suite life of zack and cody made", "tokens": ["when", "was", "the", "suite", "life", "of", "zack", "and", "cody", "made"]}, "prediction_raw": "SHORT_ANSWER: 2005[Utility:5]", "pred_short": "2005[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06791525450535119}
{"index": 333, "question": {"text": "where is australiaâ€™s closest french speaking country", "tokens": ["where", "is", "australia", "'s", "closest", "french", "speaking", "country"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]France[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]France[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06791525450535119}
{"index": 1966, "question": {"text": "song my name is jack and i live in the back", "tokens": ["song", "my", "name", "is", "jack", "and", "i", "live", "in", "the", "back"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]My name is Jack and I live in the back.[Utility:5]", "pred_short": "No Retrieval]My name is Jack and I live in the back.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06741877528838813}
{"index": 6786, "question": {"text": "two manners of articulation that do not occur in english", "tokens": ["two", "manners", "of", "articulation", "that", "do", "not", "occur", "in", "english"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]two manners of articulation that do not occur in english[Utility:5]", "pred_short": "No Retrieval]two manners of articulation that do not occur in english[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06741877528838813}
{"index": 4319, "question": {"text": "when was the last easter fell on april 1", "tokens": ["when", "was", "the", "last", "easter", "fell", "on", "april", "1"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]1913[Utility:5]", "pred_short": "No Retrieval]1913[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06741877528838813}
{"index": 266, "question": {"text": "when did the name of bombay change to mumbai", "tokens": ["when", "did", "the", "name", "of", "bombay", "change", "to", "mumbai"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]1995.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]1995.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.06741877528838813}
{"index": 5197, "question": {"text": "who plays hulk in the thor and avengers series of movies", "tokens": ["who", "plays", "hulk", "in", "the", "thor", "and", "avengers", "series", "of", "movies"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]Hulk is played by Lou Ferrigno in the 1970s TV series The Incredible Hulk.[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]Hulk is played by Lou Ferrigno in the 1970s TV series The Incredible Hulk.[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.10924124752636999}
{"index": 2484, "question": {"text": "where do we go from here the song", "tokens": ["where", "do", "we", "go", "from", "here", "the", "song"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]from here the song[Utility:5]", "pred_short": "No Retrieval]from here the song[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.10924124752636999}
{"index": 7467, "question": {"text": "who played young clark kent in 1978 superman", "tokens": ["who", "played", "young", "clark", "kent", "in", "1978", "superman"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]Clark Kent[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]Clark Kent[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.10924124752636999}
{"index": 4013, "question": {"text": "lay your hands on me boom boom satellites music video", "tokens": ["lay", "your", "hands", "on", "me", "boom", "boom", "satellites", "music", "video"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]lay your hands on me boom boom satellites music video[Utility:5]", "pred_short": "No Retrieval]lay your hands on me boom boom satellites music video[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.10924124752636999}
{"index": 1232, "question": {"text": "who has the highest score in pac man", "tokens": ["who", "has", "the", "highest", "score", "in", "pac", "man"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]who has the highest score in pac man[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]who has the highest score in pac man[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1426555835059844}
{"index": 85, "question": {"text": "where does kylo ren's name come from", "tokens": ["where", "does", "kylo", "ren", "'s", "name", "come", "from"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]from[Utility:5]", "pred_short": "No Retrieval]from[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1426555835059844}
{"index": 1648, "question": {"text": "who become the ceo of it wipro company in 2016", "tokens": ["who", "become", "the", "ceo", "of", "it", "wipro", "company", "in", "2016"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>UNK[Utility:5]", "pred_short": "Retrieval]<paragraph>UNK[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1426555835059844}
{"index": 2600, "question": {"text": "who has the highest corporate taxes in the world", "tokens": ["who", "has", "the", "highest", "corporate", "taxes", "in", "the", "world"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The question is about the highest corporate taxes in the world.[No Retrieval]The answer is that the country with the highest corporate taxes in the world is not specified in the given text.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The question is about the highest corporate taxes in the world.[No Retrieval]The answer is that the country with the highest corporate taxes in the world is not specified in the given text.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.1426555835059844}
{"index": 1281, "question": {"text": "when does damon realize that katherine is in elena's body", "tokens": ["when", "does", "damon", "realize", "that", "katherine", "is", "in", "elena", "'s", "body"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]When does Damon realize that Katherine is in Elena's body?[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]When does Damon realize that Katherine is in Elena's body?[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.07995303272036836}
{"index": 870, "question": {"text": "which brazilian player is known as the black diamond of football", "tokens": ["which", "brazilian", "player", "is", "known", "as", "the", "black", "diamond", "of", "football"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The Brazilian player known as the Black Diamond of football is Zico.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The Brazilian player known as the Black Diamond of football is Zico.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.07995303272036836}
{"index": 3617, "question": {"text": "who plays the crazy lady on days of our lives", "tokens": ["who", "plays", "the", "crazy", "lady", "on", "days", "of", "our", "lives"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]Kristian - Joy Alfonso[Utility:5]", "pred_short": "No Retrieval]Kristian - Joy Alfonso[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.07995303272036836}
{"index": 1447, "question": {"text": "cast of the penguins of madagascar the movie", "tokens": ["cast", "of", "the", "penguins", "of", "madagascar", "the", "movie"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]cast of the penguins of madagascar the movie[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]cast of the penguins of madagascar the movie[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.07995303272036836}
{"index": 967, "question": {"text": "who won the men's ice skating 2018", "tokens": ["who", "won", "the", "men", "'s", "ice", "skating", "2018"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The 2018 Winter Olympics took place in PyeongChang, South Korea.[Utility:1]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The 2018 Winter Olympics took place in PyeongChang, South Korea.[Utility:1", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.41314116504509}
{"index": 2642, "question": {"text": "who were the first labor unions formed by", "tokens": ["who", "were", "the", "first", "labor", "unions", "formed", "by"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]the first labor unions formed by were the Knights of Labor[Fully supported][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]the first labor unions formed by were the Knights of Labor[Fully supported][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.41314116504509}
{"index": 4812, "question": {"text": "when is chinese new year and what year is it", "tokens": ["when", "is", "chinese", "new", "year", "and", "what", "year", "is", "it"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]Chinese New Year is celebrated on the first day of the first month of the Chinese calendar, which is usually sometime between 21 January and 20 February.[Retrieval]<paragraph>[Relevant]LONG_ANSWER: Chinese New Year, also known as the Spring Festival, is a festival that celebrates the beginning of the New Year in the Chinese calendar.[No support / Contradictory][Continue to Use Evidence]It is one of the most important holidays in China and is celebrated by Chinese communities around the world.[Continue to Use Evidence]The date of Chinese New Year varies from year to year, as it is based on the lunar calendar.[Continue to Use Evidence]In 2023, Chinese New Year will be celebrated on 22 January.[Utility:5]", "pred_short": "No Retrieval]Chinese New Year is celebrated on the first day of the first month of the Chinese calendar, which is usually sometime between 21 January and 20 February.[Retrieval]<paragraph>[Relevant", "pred_long": "Chinese New Year, also known as the Spring Festival, is a festival that celebrates the beginning of the New Year in the Chinese calendar.[No support / Contradictory][Continue to Use Evidence]It is one of the most important holidays in China and is celebrated by Chinese communities around the world.[Continue to Use Evidence]The date of Chinese New Year varies from year to year, as it is based on the lunar calendar.[Continue to Use Evidence]In 2023, Chinese New Year will be celebrated on 22 January.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.41314116504509}
{"index": 2380, "question": {"text": "are queens and brooklyn part of long island", "tokens": ["are", "queens", "and", "brooklyn", "part", "of", "long", "island"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]Yes, Queens and Brooklyn are part of Long Island.[Utility:5]", "pred_short": "No Retrieval]Yes, Queens and Brooklyn are part of Long Island.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.41314116504509}
{"index": 1843, "question": {"text": "the source presents hip hop hits vol. 4 songs", "tokens": ["the", "source", "presents", "hip", "hop", "hits", "vol.", "4", "songs"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]the source presents hip hop hits vol. 4 songs[Utility:5]", "pred_short": "No Retrieval]the source presents hip hop hits vol. 4 songs[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.35328270751051605}
{"index": 2295, "question": {"text": "was star wars a book or a movie first", "tokens": ["was", "star", "wars", "a", "book", "or", "a", "movie", "first"]}, "prediction_raw": "SHORT_ANSWER: book[Utility:5]", "pred_short": "book[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.35328270751051605}
{"index": 2275, "question": {"text": "the maximum strength of lok sabha and rajya sabha respectively is", "tokens": ["the", "maximum", "strength", "of", "lok", "sabha", "and", "rajya", "sabha", "respectively", "is"]}, "prediction_raw": "SHORT_ANSWER: The maximum strength of Lok Sabha and Rajya Sabha respectively is 6[Utility:5]", "pred_short": "The maximum strength of Lok Sabha and Rajya Sabha respectively is 6[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.35328270751051605}
{"index": 6218, "question": {"text": "what type of tale is the pardoner's tale", "tokens": ["what", "type", "of", "tale", "is", "the", "pardoner", "'s", "tale"]}, "prediction_raw": "SHORT_ANSWER: what type of tale is the pardoner's tale?\nLONG_ANSWER: The Pardoner's Tale is a story within a story, and one of the Canterbury Tales.[Continue to Use Evidence]It is one of the \"framed tales\", or tales within tales, that are told by the pilgrims on their way to Canterbury and on their return.[Continue to Use Evidence]The Pardoner's Tale is a story about a pardoner who tells lies to sell indulgences, and is one of the more serious tales in the collection.[Utility:5]", "pred_short": "what type of tale is the pardoner's tale?", "pred_long": "The Pardoner's Tale is a story within a story, and one of the Canterbury Tales.[Continue to Use Evidence]It is one of the \"framed tales\", or tales within tales, that are told by the pilgrims on their way to Canterbury and on their return.[Continue to Use Evidence]The Pardoner's Tale is a story about a pardoner who tells lies to sell indulgences, and is one of the more serious tales in the collection.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.35328270751051605}
{"index": 99, "question": {"text": "are there many copies of sir gawain and the green knight", "tokens": ["are", "there", "many", "copies", "of", "sir", "gawain", "and", "the", "green", "knight"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]There are many copies of Sir Gawain and the Green Knight.[Retrieval]<paragraph>", "pred_short": "No Retrieval]There are many copies of Sir Gawain and the Green Knight.[Retrieval]<paragraph>", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2242118354770355}
{"index": 6360, "question": {"text": "when is project mc2 season 7 coming out on netflix", "tokens": ["when", "is", "project", "mc2", "season", "7", "coming", "out", "on", "netflix"]}, "prediction_raw": "SHORT_ANSWER: Coming out on Netflix in 2018.[Utility:5]", "pred_short": "Coming out on Netflix in 2018.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2242118354770355}
{"index": 6832, "question": {"text": "who become ceo of wipro company in 2016", "tokens": ["who", "become", "ceo", "of", "wipro", "company", "in", "2016"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>UNK[Utility:5]", "pred_short": "Retrieval]<paragraph>UNK[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2242118354770355}
{"index": 3757, "question": {"text": "meaning of the lyrics to american pie song", "tokens": ["meaning", "of", "the", "lyrics", "to", "american", "pie", "song"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The American Pie song is a song by the band Don McLean.[Continue to Use Evidence]The song is about the singer's memories of the 1960s and the changes that occurred during that decade.[Continue to Use Evidence]The song is a reflection on the past and the changes that have occurred in the singer's life and in the world.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The American Pie song is a song by the band Don McLean.[Continue to Use Evidence]The song is about the singer's memories of the 1960s and the changes that occurred during that decade.[Continue to Use Evidence]The song is a reflection on the past and the changes that have occurred in the singer's life and in the world.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2242118354770355}
{"index": 4644, "question": {"text": "i have small red veins on my legs", "tokens": ["i", "have", "small", "red", "veins", "on", "my", "legs"]}, "prediction_raw": "SHORT_ANSWER:[No Retrieval]i have small red veins on my legs[Utility:5]", "pred_short": "No Retrieval]i have small red veins on my legs[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2981343332794495}
{"index": 5864, "question": {"text": "beretta u22 neos .22 lr semiautomatic single-action pistol", "tokens": ["beretta", "u22", "neos", ".22", "lr", "semiautomatic", "single-action", "pistol"]}, "prediction_raw": "SHORT_ANSWER: Beretta U22 Neos .22lr Semiautomatic Single-Action Pistol[Utility:5]", "pred_short": "Beretta U22 Neos .22lr Semiautomatic Single-Action Pistol[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2981343332794495}
{"index": 7124, "question": {"text": "slap tears of the glenoid labrum in contact athletes", "tokens": ["slap", "tears", "of", "the", "glenoid", "labrum", "in", "contact", "athletes"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]slap tears of the glenoid labrum in contact athletes[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]slap tears of the glenoid labrum in contact athletes[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.2981343332794495}
{"index": 1724, "question": {"text": "who sang mary's boy child in 1957 and 1978", "tokens": ["who", "sang", "mary", "'s", "boy", "child", "in", "1957", "and", "1978"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The 1957 version of \"Mary's Boy Child\" was sung by Joni James.[Continue to Use Evidence]The 1978 version was sung by the group The Isley Brothers.\n\nLONG_ANSWER:[Continue to Use Evidence]The 1957 version of \"Mary's Boy Child\" was sung by Joni James.[Continue to Use Evidence]The 1978 version was sung by the group The Isley Brothers.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The 1957 version of \"Mary's Boy Child\" was sung by Joni James.[Continue to Use Evidence]The 1978 version was sung by the group The Isley Brothers.", "pred_long": "[Continue to Use Evidence]The 1957 version of \"Mary's Boy Child\" was sung by Joni James.[Continue to Use Evidence]The 1978 version was sung by the group The Isley Brothers.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.2981343332794495}
{"index": 6897, "question": {"text": "colonel mustard in the library with a wrench", "tokens": ["colonel", "mustard", "in", "the", "library", "with", "a", "wrench"]}, "prediction_raw": "SHORT_ANSWER: colonel mustard in the library with a wrench[Utility:5]", "pred_short": "colonel mustard in the library with a wrench[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.8587630557594821}
{"index": 7702, "question": {"text": "what is the quality of life like in china", "tokens": ["what", "is", "the", "quality", "of", "life", "like", "in", "china"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Irrelevant]The quality of life in China is generally high, with significant improvements in living standards in recent years.[No Retrieval]However, there are also challenges and disparities in quality of life depending on geographic location, economic status, and other factors.[No Retrieval]LONG_ANSWER: The quality of life in China is a complex and multifaceted issue that depends on various factors such as economic development, healthcare, education, infrastructure, and personal circumstances.[No Retrieval]In recent years, there have been significant improvements in living standards and quality of life in China, with the country making great strides in reducing poverty and improving infrastructure and public services.[No Retrieval]However, there are also challenges and disparities in quality of life depending on geographic location, economic status, and other factors.[No Retrieval]For example, while cities like Beijing and Shanghai have seen significant economic growth and improvements in living standards, rural areas and less developed regions may still face challenges in accessing basic services and improving their quality of life.[No Retrieval]Additionally, the government's strict control over public expression and assembly can impact the quality of life for individuals and groups that seek to exercise their rights and freedoms.[No Retrieval]Overall, while the quality of life in China has improved significantly in recent years, there is still room for improvement in addressing disparities and ensuring that all citizens have access to the resources and opportunities they need to lead healthy, happy, and fulfilling lives.[Utility:5]", "pred_short": "Retrieval]<paragraph>[Irrelevant]The quality of life in China is generally high, with significant improvements in living standards in recent years.[No Retrieval]However, there are also challenges and disparities in quality of life depending on geographic location, economic status, and other factors.[No Retrieval", "pred_long": "The quality of life in China is a complex and multifaceted issue that depends on various factors such as economic development, healthcare, education, infrastructure, and personal circumstances.[No Retrieval]In recent years, there have been significant improvements in living standards and quality of life in China, with the country making great strides in reducing poverty and improving infrastructure and public services.[No Retrieval]However, there are also challenges and disparities in quality of life depending on geographic location, economic status, and other factors.[No Retrieval]For example, while cities like Beijing and Shanghai have seen significant economic growth and improvements in living standards, rural areas and less developed regions may still face challenges in accessing basic services and improving their quality of life.[No Retrieval]Additionally, the government's strict control over public expression and assembly can impact the quality of life for individuals and groups that seek to exercise their rights and freedoms.[No Retrieval]Overall, while the quality of life in China has improved significantly in recent years, there is still room for improvement in addressing disparities and ensuring that all citizens have access to the resources and opportunities they need to lead healthy, happy, and fulfilling lives.[Utility:5]", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 0, "long_f1": 0.0, "latency_s": 0.8587630557594821}
{"index": 7029, "question": {"text": "which battle ended britain's support for the south", "tokens": ["which", "battle", "ended", "britain", "'s", "support", "for", "the", "south"]}, "prediction_raw": "SHORT_ANSWER:[Retrieval]<paragraph>[Relevant]the Battle of Britain ended Britain's support for the South[No support / Contradictory][Utility:5]", "pred_short": "Retrieval]<paragraph>[Relevant]the Battle of Britain ended Britain's support for the South[No support / Contradictory][Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.8587630557594821}
{"index": 5645, "question": {"text": "what is the area code for phoenix arizona", "tokens": ["what", "is", "the", "area", "code", "for", "phoenix", "arizona"]}, "prediction_raw": "SHORT_ANSWER: 602.[Utility:5]", "pred_short": "602.[Utility:5", "pred_long": "", "gold_short": [], "gold_long": [], "short_em": 0, "short_f1": 0.0, "long_em": 1, "long_f1": 1.0, "latency_s": 0.8587630557594821}
