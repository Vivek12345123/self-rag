avg_latency_s,dataset,em,evidence_f1_micro,evidence_precision_micro,evidence_recall_micro,examples_evaluated,f1,fever_score,label_accuracy,note,official_predictions_file,output_file,response_f1,response_precision,response_recall,span_prf_avg,span_prf_computed,split
0.1344915484264493,mwong/fever-evidence-related,,0.0,0.0,0.0,100,,0.48,0.48,,,selfrag_eval_smoke_100/fever_outputs.jsonl,,,,,,train
,microsoft/ms_marco,,,,,0,,,,No evaluable examples found (no ranked retrieval fields detected).,,selfrag_eval_smoke_100/microsoft_ms_marco_outputs.jsonl,,,,,,validation
0.29655623365193606,squad_v2,0.36,,,,100,0.10300380458660659,,,,,selfrag_eval_smoke_100/squad_v2_outputs.jsonl,,,,,,validation
,hotpotqa/hotpot_qa__distractor,,,,,100,,,,,,selfrag_eval_smoke_100/hotpotqa_hotpot_qa__distractor_outputs.jsonl,,,,,,validation
,hotpotqa/hotpot_qa__fullwiki,,,,,100,,,,,,selfrag_eval_smoke_100/hotpotqa_hotpot_qa__fullwiki_outputs.jsonl,,,,,,validation
0.7246490641939454,wandb/RAGTruth-processed,,,,,100,,,,,,selfrag_eval_smoke_100/ragtruth_outputs.jsonl,0.25263157894736843,0.20689655172413793,0.32432432432432434,,False,train
,mandarjoshi/trivia_qa::rc,,,,,100,,,,,,selfrag_eval_smoke_100/mandarjoshi_trivia_qa__rc_outputs.jsonl,,,,,,validation
0.38166242636507375,google-research-datasets/natural_questions,,,,,100,,,,,,selfrag_eval_smoke_100/natural-questions_outputs.jsonl,,,,,,validation
