accuracy,avg_latency_s,dataset,em,examples_evaluated,f1,fever_label_mode,inclusion_rate,note,output_file,response_f1,response_precision,response_recall,span_prf_avg,span_prf_computed,split
0.58,0.12942230376880615,mwong/fever-evidence-related,,50,,binary,,,selfrag_eval_outputs_small/fever_outputs.jsonl,,,,,,train
,,microsoft/ms_marco,,0,,,,No evaluable examples found (no ranked retrieval fields detected).,selfrag_eval_outputs_small/microsoft_ms_marco_outputs.jsonl,,,,,,validation
,0.28976361243985593,hotpotqa/hotpot_qa__distractor,0.44,50,0.09794347185308702,,,,selfrag_eval_outputs_small/hotpotqa_hotpot_qa__distractor_outputs.jsonl,,,,,,validation
,0.25999196644406763,hotpotqa/hotpot_qa__fullwiki,0.2,50,0.0746207097524674,,,,selfrag_eval_outputs_small/hotpotqa_hotpot_qa__fullwiki_outputs.jsonl,,,,,,validation
,0.7858608778053895,wandb/RAGTruth-processed,,50,,,,,selfrag_eval_outputs_small/ragtruth_outputs.jsonl,0.13636363636363635,0.10714285714285714,0.1875,,False,train
,0.2182490433845669,mandarjoshi/trivia_qa::rc,,50,,,0.4,,selfrag_eval_outputs_small/mandarjoshi_trivia_qa__rc_outputs.jsonl,,,,,,validation
,0.21351862074807287,sentence-transformers/natural-questions,0.0,50,0.07155397172776967,,,,selfrag_eval_outputs_small/sentence-transformers_natural-questions_outputs.jsonl,,,,,,train
