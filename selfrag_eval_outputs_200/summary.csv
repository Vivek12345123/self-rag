accuracy,avg_latency_s,dataset,em,examples_evaluated,f1,fever_label_mode,inclusion_rate,note,output_file,response_f1,response_precision,response_recall,span_prf_avg,span_prf_computed,split
0.435,0.1347150906070601,mwong/fever-evidence-related,,200,,binary,,,selfrag_eval_outputs_200/fever_outputs.jsonl,,,,,,train
,,microsoft/ms_marco,,0,,,,No evaluable examples found (no ranked retrieval fields detected).,selfrag_eval_outputs_200/microsoft_ms_marco_outputs.jsonl,,,,,,validation
,0.32537517700344326,hotpotqa/hotpot_qa__distractor,0.46,200,0.10620616304089311,,,,selfrag_eval_outputs_200/hotpotqa_hotpot_qa__distractor_outputs.jsonl,,,,,,validation
,0.38775418988312593,hotpotqa/hotpot_qa__fullwiki,0.28,200,0.072880205822524,,,,selfrag_eval_outputs_200/hotpotqa_hotpot_qa__fullwiki_outputs.jsonl,,,,,,validation
,0.6679025764658582,wandb/RAGTruth-processed,,200,,,,,selfrag_eval_outputs_200/ragtruth_outputs.jsonl,0.3212435233160622,0.2743362831858407,0.3875,,False,train
,0.17684666635352186,mandarjoshi/trivia_qa::rc,,200,,,0.4,,selfrag_eval_outputs_200/mandarjoshi_trivia_qa__rc_outputs.jsonl,,,,,,validation
,0.2392139507632237,sentence-transformers/natural-questions,0.0,200,0.07966856774527296,,,,selfrag_eval_outputs_200/sentence-transformers_natural-questions_outputs.jsonl,,,,,,train
